{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c17a2c",
   "metadata": {},
   "source": [
    "Creating a Convolutional Neural Network (CNN) in TensorFlow for an image classification task using your own dataset is a common use case. Below is an example of how to implement a CNN for image classification using TensorFlow and Keras, assuming you have a dataset of images organized in folders where each folder represents a different class (commonly known as the \"image carpet\" structure).\n",
    "\n",
    "\n",
    "Creating a Convolutional Neural Network (CNN) in TensorFlow for an image classification task using your own dataset is a common use case. Below is an example of how to implement a CNN for image classification using TensorFlow and Keras, assuming you have a dataset of images organized in folders where each folder represents a different class (commonly known as the \"image carpet\" structure).\n",
    "\n",
    "Let's assume the following directory structure for your dataset:\n",
    "\n",
    "```\n",
    "- dataset/\n",
    "  - class_1/\n",
    "    - image_1.jpg\n",
    "    - image_2.jpg\n",
    "    ...\n",
    "  - class_2/\n",
    "    - image_1.jpg\n",
    "    - image_2.jpg\n",
    "    ...\n",
    "  ...\n",
    "```\n",
    "\n",
    "Here's a code example to implement a CNN for this dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a606bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf2c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "input_shape = (224,224,3)  # Adjust the input shape according to your images\n",
    "num_classes = 2  # Change this to the number of classes in your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ImageDataGenerator for data augmentation and preprocessing\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,  # Rescale pixel values to [0, 1]\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # Split data into training and validation sets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776cf32e",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0373bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the absolute path to the dataset directory\n",
    "dataset_directory = os.path.abspath('C:\\\\Users\\\\52477\\\\Desktop\\\\IA_PROJECTS\\\\DATASETS\\\\xray_dataset_covid19\\\\train')\n",
    "\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(dataset_directory):\n",
    "    # List the contents of the directory\n",
    "    contents = os.listdir(dataset_directory)\n",
    "    print(\"Contents of the dataset directory:\", contents)\n",
    "else:\n",
    "    print(\"The dataset directory does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ac1f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using the flow_from_directory method\n",
    "train_generator = datagen.flow_from_directory(\n",
    "     dataset_directory\n",
    ",  # Path to the dataset directory\n",
    "    target_size=input_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Use 'categorical' for multi-class classification\n",
    "    subset='training'  # Specify 'training' or 'validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077b64af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the absolute path to the dataset directory\n",
    "dataset_directory_test = os.path.abspath('C:\\\\Users\\\\52477\\\\Desktop\\\\IA_PROJECTS\\\\DATASETS\\\\xray_dataset_covid19\\\\test')\n",
    "\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(dataset_directory):\n",
    "    # List the contents of the directory\n",
    "    contents = os.listdir(dataset_directory)\n",
    "    print(\"Contents of the dataset directory:\", contents)\n",
    "else:\n",
    "    print(\"The dataset directory does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d57671",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = datagen.flow_from_directory(\n",
    "    dataset_directory_test ,\n",
    "    target_size=input_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542f757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ae677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9963b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4383a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(validation_generator, verbose=2)\n",
    "print(\"\\nTest accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d5ee7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e688b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Define the CNN architecture\n",
    "model = models.Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c510218d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=10, \n",
    "                    validation_data=(test_images, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff181515",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(\"\\nTest accuracy:\", test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
